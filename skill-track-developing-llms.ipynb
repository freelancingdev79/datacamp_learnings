{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# DataCamp Skill Track: Develeoping Large Language Models (LLMs)\n",
    "\n",
    "**Description:** The purpose of this skill track is to learn the art of developing large language models (LLMs) with PyTorch and Hugging Face, using the latest deep learning and NLP techniques. There are a total of 6 courses and the total duration of this track is $16$ hours.\n",
    "\n",
    "**Reference** [Skill track: Developing Large Language Models](https://app.datacamp.com/learn/skill-tracks/developing-large-language-models)\n",
    "\n",
    "**Author:** Tirthankar Dutta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Essential Settings](#essential-settings)\n",
    "  \n",
    "* [Course-1: Introduction to LLMs in Python](#course-1-introduction-to-llms-in-python)\n",
    "\n",
    "    * [Course-1.1: Getting Started with Large Language Models (LLMs)](#courese-11-getting-started-with-large-language-models-llms)\n",
    " \n",
    "    * [Course-1.2: Fine-tuning LLMs](#course-12-fine-tuning-llms)\n",
    " \n",
    "    * [Course-1.3: Evaluating LLM Performance](#course-13-evaluating-llm-performance)  \n",
    "\n",
    "* [Course-2: Working with Llama 3](#course-2-working-with-llama3)\n",
    "\n",
    "* [Course-3: Deep Learning for Text with PyTorch](#course-3-deep-learning-for-text-with-pytorch)\n",
    "\n",
    "* [Course-4: Transformer Models with PyTorch](#course-4-transformer-models-with-pytorch)\n",
    "\n",
    "* [Course-5: Reinforcement Learning from Human Feedback (RLHF)](#course-5-reinforcement-learning-from-human-feedback-rhlf)\n",
    "\n",
    "* [Course-6: LLMOps Concepts](#course-6-llmops-concepts)\n",
    "\n",
    "* [Project-1: Analyzing Car Reviews with LLMs](#project-1-analyzing-car-reviews-with-llms)\n",
    "\n",
    "* [Project-2: Classifying Emails using Llama](#project-2-classifying-emails-with-llama)\n",
    "\n",
    "* [Project-3: Service Desk Ticket Classification with Deep Learning](#project-3-service-desk-ticket-classification-with-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Huggingface Models on Corporate Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this section is to explore approaches to work with Hugging Face models on corporate devices. A corporate device (laptop/desktop) has a variety of restrictions applicable to it due to organizational policies. Some representative examples of restrictions are, restricted data sharing and transfer, selective internet access, etc. \n",
    "\n",
    "In one of the organizations where I have worked, company policy restricted the employees from accessing and using OpenAI API. Whenever an employee tried to access the OpenAI API using the Python SDK, a `SSLError` would be raised. This SSL errors however, could be bypassed using suitable _hacks_ available on the StackOverflow forum.\n",
    "\n",
    "However, if the organization decides to blacklist a certain website or a domain, it is impossible to bypass such restrictions. In principle an employee can raise a request to the IT team to whitelist these blocked sites but a proper business justification is required - In most cases such business justifications demands that the whitelisting requirement is related to a client project and not a personal exploration or experimentation endeavor. I faced this scenario in another organization wherein we could not use Hugging Face models for any kind of experimentation, required for downloading the models locally to a persistent storage or cache, were blacklisted. \n",
    "\n",
    "This particular obstacle prompted me to write this section in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Hugging Face models for two distinct purposes: \n",
    "\n",
    "- Inference with pretrained models\n",
    "- Inference with fine-tuned models obtained from pretrained models.\n",
    "\n",
    "For the second purpose, we need to download the model to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "device: Union[torch.device, str] = (\n",
    "    torch.cuda.get_device_name() if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text: str = \"\"\"\n",
    "Ritwik Ghatak (1925-1976) was a renowned Indian filmmaker known for his poignant storytelling and exploration of social issues, particularly the impact of the Partition of India on Bengali society.\n",
    "Early Life\n",
    "Ritwik Kumar Ghatak was born on November 4, 1925, in Dacca, Bengal Presidency, British India (now Dhaka, Bangladesh). He was the son of Suresh Chandra Ghatak, a poet and playwright, and Indubala Devi. Ghatak had a large family, with eight siblings, including the poet Manish Ghatak. After the Partition of India in 1947, his family relocated to Kolkata, which significantly influenced his work and themes in cinema.\n",
    "Career Highlights\n",
    "Ghatak began his career in the film industry as an actor and assistant director in Nimai Ghosh's Chinnamul (1950). His first completed film as a director was Nagarik (1952), which is considered a landmark in Bengali cinema. He is best known for his partition trilogy, which includes:\n",
    "Meghe Dhaka Tara (The Cloud-Capped Star, 1960)\n",
    "Komal Gandhar (E Flat, 1961)\n",
    "Subarnarekha (The Golden Thread, 1962)\n",
    "These films are celebrated for their deep emotional resonance and social realism, often reflecting the struggles of ordinary people against the backdrop of societal upheaval.\n",
    "Themes and Style\n",
    "Ghatak's films are characterized by their theatricality, documentary realism, and a strong focus on the human condition. He often explored themes of displacement, identity, and the socio-political landscape of Bengal. His work is noted for its Brechtian influences, combining stylized performances with a critical perspective on society.\n",
    "Awards and Recognition\n",
    "Despite facing challenges during his lifetime, Ghatak received several accolades for his contributions to cinema, including:\n",
    "Padma Shri in 1970 for his contributions to the arts.\n",
    "National Film Award's Rajat Kamal Award for Best Story in 1974 for Jukti Takko Aar Gappo.\n",
    "Best Director's Award from the Bangladesh Cine Journalist's Association for Titash Ekti Nadir Naam.\n",
    "Personal Life and Legacy\n",
    "Ghatak struggled with alcoholism in his later years, which affected his health and career. He passed away on February 6, 1976, in Kolkata. His legacy continues through his films, which are increasingly recognized and studied for their artistic and cultural significance. His son, Ritaban Ghatak, is also a filmmaker, and his family remains involved in preserving his memory and contributions to cinema.\n",
    "Ritwik Ghatak is now regarded as one of the greatest filmmakers in Indian cinema, alongside contemporaries like Satyajit Ray and Mrinal Sen, and his works are celebrated for their profound impact on the film industry and society at large. \n",
    "\"\"\"\n",
    "\n",
    "print(f\"Text to summarize:\\n{input_text:s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from huggingface_hub.inference import \n",
    "\n",
    "llm: HuggingFaceEndpoint = HuggingFaceEndpoint(\n",
    "    name=\"summarizer_llm\",\n",
    "    model=\"Qwen/Qwen3-Coder-480B-A35B-Instruct\",\n",
    "    temperature=0.01,\n",
    "    top_k=5,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=1024,\n",
    "    provider=\"auto\",\n",
    ")\n",
    "\n",
    "prompt: str = \"Summarize the text given below:\\n\\n{text}\\n\"\n",
    "prompt_template: PromptTemplate = PromptTemplate.from_template(template=prompt)\n",
    "# prompt_value: PromptValue = prompt_template.invoke(input=input_text)\n",
    "# print(prompt_value)\n",
    "\n",
    "output_parser: StrOutputParser = StrOutputParser()\n",
    "summary_chain: RunnableSerializable = prompt_template | llm | output_parser\n",
    "summary: str = summary_chain.invoke(input={\"text\": input_text})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first section of this notebook we execute all the necessary steps that are required for running the code cells contained in this notebook.\n",
    "\n",
    "* If we use local CPU/GPU to run this notebook, we should keep `execute = False` and select the appropriate local Jupyter kernel from the top-right corner of the notebook.\n",
    "\n",
    "* If we use Kaggle's kernel to run this notebook, we should make `execute = True` and execute the code enclosed within the `if` statement. But before doing so, we must change the kernel of this Jupyter notebook according to [these instructions](https://www.kaggle.com/code/antdes/how-to-add-kaggle-to-cursor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "from colorama import init, Fore, Style\n",
    "from pathlib import Path\n",
    "from typing import Literal, List, Union\n",
    "\n",
    "%xmode Minimal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Essential utility/helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies(\n",
    "    deps_file: Union[str, Path],\n",
    "    pkg_manager: Literal[\"uv\", \"pip\"],\n",
    ") -> None:\n",
    "    \"\"\"Install essential dependencies on the remote Kaggle kernel.\n",
    "\n",
    "    Args:\n",
    "        deps_file (Union[str, Path]): Absolute file path of the dependency file.\n",
    "        pkg_manager (Literal[\"uv\", \"pip\"]): The Python package manager used for\n",
    "        installing dependencies.\n",
    "\n",
    "    Returns:\n",
    "        None: No return values\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: Raise if Python package manager used for installing dependencies is neither `pip` nor `uv`.\n",
    "        CalledProcessError: Raise if called process exit code is not `0`.\n",
    "        Exception: Raise if some unknown/unforeseen error crashes the code.\n",
    "    \"\"\"\n",
    "    init()\n",
    "    if pkg_manager not in [\"uv\", \"pip\"]:\n",
    "        raise NotImplementedError(\n",
    "            f\"{pkg_manager} not supported by current implementation!\"\n",
    "        )\n",
    "\n",
    "    if pkg_manager == \"pip\":\n",
    "        cmd: str = (\n",
    "            \"python3 -m pip install --trusted-host pypi.org \"\n",
    "            f\"--trusted-host files.pythonhosted.org -r {deps_file} \"\n",
    "            \"--no-cache-dir\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        cmd: str = (\n",
    "            \"uv pip install --trusted-host pypi.org \"\n",
    "            f\"--trusted-host files.pythonhosted.org -r {deps_file} \"\n",
    "            \"--no-cache-dir\"\n",
    "        )\n",
    "\n",
    "    args: List[str] = cmd.split(\" \")\n",
    "\n",
    "    try:\n",
    "        result: sp.CompletedProcess = sp.run(\n",
    "            args=args,\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            shell=True,\n",
    "            text=True,\n",
    "        )\n",
    "\n",
    "        if result.check_returncode():\n",
    "            print(\n",
    "                f\"{Style.BRIGHT} {Fore.RED} Called process failed! \"\n",
    "                \"See details below...\"\n",
    "            )\n",
    "            raise sp.CalledProcessError(\n",
    "                cmd=args,\n",
    "                returncode=result.returncode,\n",
    "                output=result.stdout,\n",
    "                stderr=result.stderr,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            if result.returncode == 0:\n",
    "                print(\n",
    "                    f\"{Style.BRIGHT} {Fore.GREEN} Called process executed \"\n",
    "                    \"successfully!\"\n",
    "                )\n",
    "            if result.stdout:\n",
    "                print(f\"{Style.BRIGHT} {Fore.GREEN} StdOut: {result.stdout}\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\n",
    "            f\"{Style.BRIGHT} {Fore.RED} Called process failed due to \"\n",
    "            f\"unforeseen error! Error: {err}\\n\"\n",
    "        )\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_support() -> None:\n",
    "    \"\"\"Checks if GPU support is enables either for local/remote kernel\n",
    "    using the command `nvidia-smi`\n",
    "\n",
    "    Args:\n",
    "        No input parameter.\n",
    "\n",
    "    Returns:\n",
    "        None: No return value.\n",
    "\n",
    "    Raises:\n",
    "        CalledProcessError: Raise if called process exit code is not `0`.\n",
    "        Exception: Raise if some unknown/unforeseen error crashes the code.\n",
    "    \"\"\"\n",
    "    init()\n",
    "    cmd: str = \"nvidia-smi\"\n",
    "\n",
    "    try:\n",
    "        result: sp.CompletedProcess = sp.run(\n",
    "            args=cmd,\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            shell=True,\n",
    "            text=True,\n",
    "        )\n",
    "\n",
    "        if result.check_returncode():\n",
    "            print(\n",
    "                f\"{Style.BRIGHT} {Fore.RED} Called process failed! \"\n",
    "                \"See details below...\"\n",
    "            )\n",
    "            raise sp.CalledProcessError(\n",
    "                cmd=cmd,\n",
    "                returncode=result.returncode,\n",
    "                output=result.stdout,\n",
    "                stderr=result.stderr,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            if result.returncode == 0:\n",
    "                print(\n",
    "                    f\"{Style.BRIGHT} {Fore.GREEN} Called process executed \"\n",
    "                    \"successfully!\\n\"\n",
    "                )\n",
    "            if result.stdout:\n",
    "                print(f\"{Style.BRIGHT} {Fore.GREEN} StdOut: {result.stdout}\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\n",
    "            f\"{Style.BRIGHT} {Fore.RED} Called process failed due to \"\n",
    "            f\"unforeseen error! Error: {err}\\n\"\n",
    "        )\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute: bool = False\n",
    "if execute:\n",
    "    ROOT_DIR: Path = Path.cwd()\n",
    "    DEPS_FILE: Path = ROOT_DIR / Path(\"./pyproject.toml\")\n",
    "    print(f\"Dependency file location:\\n{DEPS_FILE}\\n\")\n",
    "    install_dependencies(deps_file=DEPS_FILE, pkg_manager=\"pip\")\n",
    "\n",
    "    # This particular line is essential to know whether this VS Code\n",
    "    # notebook is connected to the Kaggle kernel, should we choose to use\n",
    "    # one, as well as if the Kaggle kernel is using any GPU cores for\n",
    "    # acceleration.\n",
    "    print()\n",
    "    check_gpu_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course-1: Introduction to LLMs in Python\n",
    "\n",
    "**Course URL:** [Introduction to LLMs in Python](https://app.datacamp.com/learn/courses/introduction-to-llms-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course consists of the following three sub-courses:\n",
    "\n",
    "* [Course-1.1: Getting Started with Large Language Models (LLMs)](#course-11-getting-started-with-large-language-models-llms)\n",
    "\n",
    "* [Course-1.2: Fine-tuning LLMs](#course-12-fine-tuning-llms)\n",
    "\n",
    "* [Course-1.3: Evaluating LLM performance](#course-13-evaluating-llm-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course-1.1: Getting Started with Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with LLMs Using Huggingface Transformers Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course we will be working with pre-trained LLMs from the Huggingface transformers library. The code given below illustrates the process of working with LLMs using Huggingface library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text: str = \"\"\"\n",
    "Ritwik Ghatak (1925-1976) was a renowned Indian filmmaker known for his poignant storytelling and exploration of social issues, particularly the impact of the Partition of India on Bengali society.\n",
    "Early Life\n",
    "Ritwik Kumar Ghatak was born on November 4, 1925, in Dacca, Bengal Presidency, British India (now Dhaka, Bangladesh). He was the son of Suresh Chandra Ghatak, a poet and playwright, and Indubala Devi. Ghatak had a large family, with eight siblings, including the poet Manish Ghatak. After the Partition of India in 1947, his family relocated to Kolkata, which significantly influenced his work and themes in cinema.\n",
    "Career Highlights\n",
    "Ghatak began his career in the film industry as an actor and assistant director in Nimai Ghosh's Chinnamul (1950). His first completed film as a director was Nagarik (1952), which is considered a landmark in Bengali cinema. He is best known for his partition trilogy, which includes:\n",
    "Meghe Dhaka Tara (The Cloud-Capped Star, 1960)\n",
    "Komal Gandhar (E Flat, 1961)\n",
    "Subarnarekha (The Golden Thread, 1962)\n",
    "These films are celebrated for their deep emotional resonance and social realism, often reflecting the struggles of ordinary people against the backdrop of societal upheaval.\n",
    "Themes and Style\n",
    "Ghatak's films are characterized by their theatricality, documentary realism, and a strong focus on the human condition. He often explored themes of displacement, identity, and the socio-political landscape of Bengal. His work is noted for its Brechtian influences, combining stylized performances with a critical perspective on society.\n",
    "Awards and Recognition\n",
    "Despite facing challenges during his lifetime, Ghatak received several accolades for his contributions to cinema, including:\n",
    "Padma Shri in 1970 for his contributions to the arts.\n",
    "National Film Award's Rajat Kamal Award for Best Story in 1974 for Jukti Takko Aar Gappo.\n",
    "Best Director's Award from the Bangladesh Cine Journalist's Association for Titash Ekti Nadir Naam.\n",
    "Personal Life and Legacy\n",
    "Ghatak struggled with alcoholism in his later years, which affected his health and career. He passed away on February 6, 1976, in Kolkata. His legacy continues through his films, which are increasingly recognized and studied for their artistic and cultural significance. His son, Ritaban Ghatak, is also a filmmaker, and his family remains involved in preserving his memory and contributions to cinema.\n",
    "Ritwik Ghatak is now regarded as one of the greatest filmmakers in Indian cinema, alongside contemporaries like Satyajit Ray and Mrinal Sen, and his works are celebrated for their profound impact on the film industry and society at large. \n",
    "\"\"\"\n",
    "\n",
    "print(f\"Text to summarize:\\n{input_text:s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HUB_ENABLE_HF_TRANSFER=0\n",
    "!wget --no-check-certificate https://huggingface.co/google-bert/bert-base-uncased/blob/main/pytorch_model.bin hf_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "from huggingface_hub import hf_hub_download, logging\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "\n",
    "ROOT_DIR: Final[Path] = Path.cwd()\n",
    "REPO_ID: Final[str] = \"facebook/bart-large-cnn\"\n",
    "LOCAL_STORAGE_NAME: Final[str] = f\"hf_models\"\n",
    "LOCAL_STORAGE_PATH: Path = ROOT_DIR / Path(LOCAL_STORAGE_NAME)\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"facebook/bart-large-cnn\",\n",
    "    repo_type=\"model\",\n",
    "    force_download=True,\n",
    "    filename=\"pytorch_model.bin\",\n",
    "    local_dir=LOCAL_STORAGE_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "\n",
    "model: str = \"facebook/bart-large-cnn\"\n",
    "local_dir: str = str(Path.cwd() / Path(\"hf_models\"))\n",
    "if _ := os.cpu_count():\n",
    "    n_cpus: int = _\n",
    "else:\n",
    "    n_cpus = 1\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=model,\n",
    "    repo_type=\"model\",\n",
    "    local_dir=local_dir,\n",
    "    max_workers=n_cpus,\n",
    "    allow_patterns=[\"*.json\", \"*.safetensors\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import ssl\n",
    "from ctransformers.llm import LLM\n",
    "from ctransformers.hub import AutoModelForCausalLM, AutoTokenizer\n",
    "from ctransformers.transformers import CTransformersTokenizer\n",
    "from huggingface_hub import configure_http_backend\n",
    "\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "def backend_factory() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    session.verify = False\n",
    "    return session\n",
    "\n",
    "\n",
    "configure_http_backend(backend_factory=backend_factory)\n",
    "\n",
    "llm: LLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_repo_id=\"facebook/bart-large-cnn\",\n",
    "    model_file=\"pytorch_model.bin\",\n",
    "    hf=True,\n",
    ")\n",
    "\n",
    "tokenizer: CTransformersTokenizer = AutoTokenizer.from_pretrained(model=model)\n",
    "\n",
    "summary = llm(\n",
    "    prompt=input_text,\n",
    "    max_new_tokens=1024,\n",
    "    top_k=10,\n",
    "    top_p=0.9,\n",
    "    temperature=0.01,\n",
    "    threads=os.cpu_count(),\n",
    ")\n",
    "print(type(summary))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient, SummarizationOutput\n",
    "\n",
    "client: InferenceClient = InferenceClient()\n",
    "summary: SummarizationOutput = client.summarization(\n",
    "    text=input_text,\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "print(type(summary))\n",
    "print(summary)\n",
    "print(summary.summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from huggingface_hub.hf_file_system import HfFileSystem\n",
    "\n",
    "hf_repo_file_system: HfFileSystem = HfFileSystem()\n",
    "\n",
    "# List files in a HF repo file system.\n",
    "files_from_approach1: List[str] = hf_repo_file_system.glob(\n",
    "    path=\"facebook/bart-large-cnn/*.*\"\n",
    ")\n",
    "print(f\"Files in HF repo:\\n\\n{files_from_approach1}\\n\")\n",
    "\n",
    "# Alternative approach to list all files in a HF repo.\n",
    "\n",
    "# Here: `detail = True` - This yields a list of `Dict[str, str]` objects\n",
    "# with all metadata associated with each file.\n",
    "files_from_approach2: List[str | Dict[str, str]] = hf_repo_file_system.ls(\n",
    "    path=\"facebook/bart-large-cnn\",\n",
    "    refresh=True,\n",
    "    detail=True,\n",
    ")\n",
    "print(f\"Approach-2: All files:\\n\\n{files_from_approach2}\\n\")\n",
    "\n",
    "# Here: `detail = False` - This yields a list of `str` objects that gives\n",
    "# the name of each file. No metadata associated with the files are returned.\n",
    "files_from_approach2: List[str | Dict[str, str]] = hf_repo_file_system.ls(\n",
    "    path=\"facebook/bart-large-cnn\",\n",
    "    refresh=True,\n",
    "    detail=False,\n",
    ")\n",
    "print(f\"Approach-2: All files:\\n\\n{files_from_approach2}\\n\")\n",
    "\n",
    "assert all(files_from_approach1) == all(files_from_approach2), (\n",
    "    \"Output from `.ls()` method with `detail=False` and from `.glob()` must \"\n",
    "    \"be identical!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in a HF repo and then write the appropriate files to a local\n",
    "# storage.\n",
    "import json\n",
    "from io import FileIO\n",
    "from typing import Final, Union\n",
    "from pathlib import Path\n",
    "\n",
    "required_files: List[str] = [\n",
    "    \"facebook/bart-large-cnn/config.json\",\n",
    "    \"facebook/bart-large-cnn/model.safetensors\",\n",
    "    \"facebook/bart-large-cnn/pytorch_model.bin\",\n",
    "    \"facebook/bart-large-cnn/tokenizer.json\",\n",
    "    \"facebook/bart-large-cnn/vocab.json\",\n",
    "]\n",
    "\n",
    "ROOT_DIR: Final[Path] = Path.cwd()\n",
    "REPO_ID: Final[str] = \"facebook/bart-large-cnn\"\n",
    "LOCAL_STORAGE_NAME: Final[str] = f\"hf_models\"\n",
    "LOCAL_STORAGE_PATH: Path = ROOT_DIR / Path(LOCAL_STORAGE_NAME)\n",
    "\n",
    "print(f\"Local storage:\\n{LOCAL_STORAGE_PATH}\\n\")\n",
    "\n",
    "for file in required_files[:2]:\n",
    "    local_file = (LOCAL_STORAGE_PATH / Path(file)).absolute()\n",
    "    try:\n",
    "        print(f\"Downloading {file}...\")\n",
    "        hf_repo_file_system.get_file(\n",
    "            rpath=file,\n",
    "            lpath=local_file,\n",
    "        )\n",
    "        print(f\"{file} downloaded successfully...\")\n",
    "        print()\n",
    "    except Exception as err:\n",
    "        print(f\"Downloading of {file} failed! Error: {err}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_HUB_DISABLE_SYMLINKS_WARNING = 1\n",
    "snapshot_download(\n",
    "    repo_id=\"facebook/bart-cnn-large\",\n",
    "    local_dir=LOCAL_STORAGE_PATH,\n",
    "    force_download=True,\n",
    "    local_dir_use_symlinks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import adapters\n",
    "from adapters import AutoAdapterModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.pipelines import pipeline\n",
    "from transformers.pipelines.text2text_generation import SummarizationPipeline\n",
    "\n",
    "# Refer to the given StackOverflow post for details about the following line.\n",
    "# \"https://stackoverflow.com/questions/76707715/stucking-at-downloading-shards-for-loading-llm-model-from-huggingface\"\n",
    "HF_HUB_ENABLE_HF_TRANSFER = 1\n",
    "\n",
    "# model: Union[Any, None] = AutoAdapterModel.from_pretrained(\n",
    "#     pretrained_model_name_or_path=\"facebook/bart-large-cnn\",\n",
    "# )\n",
    "\n",
    "# _adaptor = model.load_adapter()\n",
    "\n",
    "local_model = AutoModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=str(\n",
    "        Path.cwd() / Path(\"Qwen2-72B-Instruct-Q8_0.gguf\")\n",
    "    )\n",
    ")\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=str(\n",
    "        Path.cwd() / Path(\"Qwen2-72B-Instruct-Q8_0.gguf\")\n",
    "    )\n",
    ")\n",
    "\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=local_model,\n",
    "    tokenizer=local_tokenizer,\n",
    ")\n",
    "summary: List[Dict[str, str]] = summarizer(input_text, max_length=100)\n",
    "print(f\"Bare Summary Object:\\n{summary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove unwanted white spaces introduced through tokenization, we can add the parameter `clean_up_tokenization_spaces` to the `pipeline` function and set its value to `True` as shown.\n",
    "\n",
    "```python\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\", \n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "\n",
    "input_text: str = \"\"\"...\"\"\"\n",
    "\n",
    "summary: str = summarizer(input_text, max_length=100)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we intend to use the Kaggle kernel to run this VS Code notebook, we must \n",
    "# make `run_code = True` to ensure that the necessary dependencies are \n",
    "# installed.\n",
    "\n",
    "run_code: bool = False \n",
    "if run_code:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course-1.2: Fine-tuning LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course-1.3: Evaluating LLM performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course-2: Working with Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course-3: Deep Learning for Text with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course-4: Transformer Models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course-5: Reinforcement Learning from Human Feedback (RHLF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course-6: LLMOps Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project-1: Analyzing Car Reviews with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project-2: Classifying Emails using Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project-3: Service Desk Ticket Classification with Deep Learning"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".datacamp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
